\exercise{Policy Gradient}
In this exercise, you are going to solve the same task of the previous exercise but using policy gradient. 
For the programming exercises, you are allowed to reuse part of the EM code. 
Attach snippets of your code.

\begin{questions}

%----------------------------------------------

\begin{question}{Analytical Derivation}{5}
 	You have a Gaussian policy with diagonal covariance, i.e., $\pi(\vec \theta | \vec \omega) = \gauss{\vec \mu}{ \diag(\vec \sigma^2)}$, where $\vec \omega = [\vec \mu,\vec \sigma]$.
 	Compute analytically the gradient of the logarithm of the policy with respect to the parameters $\vec\omega$, i.e., $\nabla_{\vec\omega} \log \pi(\vec \theta | \vec \omega)$. (Hint: consider the properties of the diagonal covariance matrix.)
 	
 	\begin{answer}

	\end{answer}

\end{question}

%----------------------------------------------

\begin{question}{Programming Exercise}{5}
	Consider the same robotic task as in the previous exercise and this time solve it with policy gradient. 
	Use an initial mean of $\vec \mu_0 = [0 \dots 0]$ and a fixed $\vec \sigma = \diag([10 \dots 10])$ (i.e., do \textbf{not} update $\vec\sigma$). Set the learning rate to $\alpha=0.1$ and use the same hyperparameters as before (25 episodes sampled for each iteration and max 100 iterations).\\
	Repeat the learning 10 times and plot the mean of the average return of all runs with $95\%$ confidence.
	Use the logarithmic scale for your plot. Comment your results.
	
	\begin{answer}

	\end{answer}
\end{question}


%----------------------------------------------


\begin{question}{A Little Trick}{5}
	How would you improve the above implementation? (Beside using a smaller or adaptive learning rate, or using the natural gradient). What is the theory behind this ``trick''? Repeat the learning and discuss the results.
	
	\begin{answer}

	\end{answer}
\end{question}
	

%----------------------------------------------

\begin{question}{Learning Rate}{5}
	Repeat the optimization changing the learning rate to $\alpha=0.4$ and $\alpha = 0.2$ (keep the trick of the previous exercise).
	Plot in one figure the mean of the average returns for all $\alpha$ with $95\%$ confidence.
	How does the value of $\alpha$ affect the convergence of the algorithm? 
	Use the logarithmic scale for your plot.
		
	\begin{answer}

	\end{answer}
\end{question}

%----------------------------------------------
	
\begin{question}{Variable Variance}{5}
	Try to improve the optimization process by learning also the variance $\vec \sigma$. Is it easier or harder to learn also the variance? Why?
	\\Without using the natural gradient, tune the learning process to \textbf{achieve better results}. If you think it is necessary, you can impose a lower bound to avoid that the variance collapses to infinitely small values (e.g., if $\sigma(i)<\sigma_{\text{lower}}$ then $\sigma(i) = \sigma_{\text{lower}}$).
	In one figure, plot the learning trend with confidence interval as done before and compare it to the one achieved with $\alpha = 0.4$ before.
	
	\begin{answer}

	\end{answer}
\end{question}

%----------------------------------------------

	
\begin{question}[bonus]{Natural Gradient}{5}
	Write down the equation of the natural gradient. What is the theory behind it?
	Is it always easy to use?
	
	\begin{answer}

	\end{answer}
\end{question}

\end{questions}